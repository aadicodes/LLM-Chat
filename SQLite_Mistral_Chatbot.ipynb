{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMV0gT5J2cBEVVMAGfhHRAW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHHJTDOcMRO9",
        "outputId": "22c1d544-d0f7-4559-9d8e-d616d2e9097d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers sentence-transformers gradio datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "#enterprise_data.db\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "\n",
        "conn = sqlite3.connect(\"enterprise_data.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Tables in database:\", tables)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3ulyaWQNCJO",
        "outputId": "9f1163ae-d186-4633-8c10-82c551c9d419"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tables in database: [('branch_revenue',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = pd.DataFrame([\n",
        "    {'branch_name': 'Miami Branch', 'revenue': 72000.00, 'quarter': 'Q2', 'year':2025},\n",
        "    {'branch_name': 'Dallas Branch', 'revenue': 88000.00, 'quarter': 'Q2', 'year':2025},\n",
        "    {'branch_name': 'Phoenix Branch', 'revenue': 188000.00, 'quarter': 'Q3', 'year':2025},\n",
        "    {'branch_name': 'Austin Branch', 'revenue': 128000.00, 'quarter': 'Q3', 'year':2025},\n",
        "    {'branch_name': 'Denver Branch', 'revenue': 126000.00, 'quarter': 'Q3', 'year':2025}\n",
        "])\n",
        "\n",
        "df_new.to_sql('branch_revenue', conn, if_exists='append', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Wf7uC_Ui69",
        "outputId": "07862881-efc0-4281-c186-81dc2531c4ce"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Connect to uploaded SQLite file\n",
        "conn = sqlite3.connect(\"enterprise_data.db\")  # Replace with your file name\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load table data\n",
        "df = pd.read_sql_query(\"SELECT * FROM branch_revenue\", conn)\n",
        "print(df)\n",
        "conn.close()\n",
        "\n",
        "# Embed rows\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "corpus = df.astype(str).agg(\" \".join, axis=1).tolist()\n",
        "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AphdhA9IOPv1",
        "outputId": "2d251058-36c2-41a3-b472-399d55048aa7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      branch_name quarter  year    revenue\n",
            "0         Phoenix      Q2  2025  4200000.0\n",
            "1          Austin      Q2  2025  3800000.0\n",
            "2          Denver      Q2  2025  3500000.0\n",
            "3    Miami Branch      Q2  2025    72000.0\n",
            "4   Dallas Branch      Q2  2025    88000.0\n",
            "5  Phoenix Branch      Q3  2025   188000.0\n",
            "6   Austin Branch      Q3  2025   128000.0\n",
            "7   Denver Branch      Q3  2025   126000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # No token required\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "chatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S1a7JLQOUSA",
        "outputId": "18a4fb66-469e-49a0-dcbd-87931a2b3ef5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(user_query):\n",
        "    query_embedding = embedder.encode(user_query, convert_to_tensor=True)\n",
        "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)[0]\n",
        "    context = \"\\n\".join([corpus[hit['corpus_id']] for hit in hits])\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{user_query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    response = chatbot(prompt, max_new_tokens=200, do_sample=True)[0]['generated_text']\n",
        "    return response.split(\"Answer:\")[-1].strip()"
      ],
      "metadata": {
        "id": "CrSOJk2qOXXo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.Interface(fn=answer_question, inputs=\"text\", outputs=\"text\",\n",
        "             title=\"ğŸ“Š Chatbot using Mistral LLM\",\n",
        "             description=\"Ask questions about your enterprise data stored in SQLite.\").launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "lWD_JfgtOhUJ",
        "outputId": "3b45d97b-ff23-4fd9-85dd-09c0fea36b17"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1d4fcc6eeb1e4af01f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1d4fcc6eeb1e4af01f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}