{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMXaKfh56zo9duz6IZxar9L"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQdiB9Hpjncv"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "# !pip install transformers accelerate streamlit --quiet\n",
        "!pip cache purge\n",
        "\n",
        "!pip install --upgrade pip setuptools wheel\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y build-essential libssl-dev libffi-dev python3-dev\n"
      ],
      "metadata": {
        "id": "VDDhbwTevWWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate text-generation"
      ],
      "metadata": {
        "id": "5JG6o_FzvOq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc647c28"
      },
      "source": [
        "# check disk space\n",
        "!df -h /\n",
        "!du -sh /root/* /usr/* /content/* 2>/dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c370015"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "# Create chatbot pipeline\n",
        "chatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check disk space\n",
        "!df -h /\n",
        "!du -sh /root/* /usr/* /content/* 2>/dev/null"
      ],
      "metadata": {
        "id": "TpzgNTtz06yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the GPU allocated in virtual environment\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "jr9veaAon2KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(user_input, domain=\"healthcare\"):\n",
        "    system_prompt = f\"You are a helpful assistant for {domain} customer service. Be accurate, empathetic and concise.\"\n",
        "    return f\"{system_prompt}\\nUser: {user_input}\\nAssistant:\"\n"
      ],
      "metadata": {
        "id": "k5aSx3k_kgFP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(prompt, max_tokens=512):\n",
        "    response = chatbot(prompt, max_new_tokens=max_tokens, do_sample=True, temperature=0.7)\n",
        "    return response[0]['generated_text']\n"
      ],
      "metadata": {
        "id": "Id9RxUVukdoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"I need help understanding my insurance coverage for a recent surgery.\"\n",
        "prompt = build_prompt(user_input, domain=\"healthcare\")\n",
        "response = get_response(prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "aIXd0jXIklHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def build_prompt(user_input, domain=\"healthcare\"):\n",
        "    system_prompt = f\"You are a helpful assistant for {domain} customer service. Be accurate, empathetic, funny and concise.\"\n",
        "    return f\"{system_prompt}\\nUser: {user_input}\\nAssistant:\"\n",
        "\n",
        "def get_response(prompt, max_tokens=512):\n",
        "    # Assuming 'chatbot' pipeline is already defined and loaded\n",
        "    # from the previous cell (3c370015)\n",
        "    response = chatbot(prompt, max_new_tokens=max_tokens, do_sample=True, temperature=0.7)\n",
        "    # Extract the generated text from the response.\n",
        "    # The pipeline output includes the input prompt, so we need to remove it.\n",
        "    generated_text = response[0]['generated_text']\n",
        "    # Find the start of the assistant's response (after the prompt)\n",
        "    assistant_start = generated_text.find(\"Assistant:\")\n",
        "    if assistant_start != -1:\n",
        "        return generated_text[assistant_start + len(\"Assistant:\"):].strip()\n",
        "    return generated_text.strip()\n",
        "\n",
        "\n",
        "def respond(user_input, chat_history, selected_domain):\n",
        "    if not user_input:\n",
        "        return chat_history, \"\"\n",
        "\n",
        "    prompt = build_prompt(user_input, selected_domain)\n",
        "    bot_response = get_response(prompt)\n",
        "\n",
        "    # Append user input and bot response to chat history\n",
        "    chat_history.append((user_input, bot_response))\n",
        "\n",
        "    # Return the updated chat history and clear the input box\n",
        "    return chat_history, \"\"\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ðŸ’¬ DeepSeek Chatbot for Finance & Healthcare - Rajesh Aadi\")\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        ### Watchout : This webapp uses AI Model, ```deepseek-coder-6.7b-instruct```,\n",
        "        not really as smart as many other AI Models out there.\n",
        "        \"\"\"\n",
        "        )\n",
        "\n",
        "\n",
        "    domain = gr.Radio([\"finance\", \"healthcare\"], label=\"Choose Domain\", value=\"finance\")\n",
        "\n",
        "    # Use Chatbot component to display conversation history\n",
        "    chatbot_output = gr.Chatbot(label=\"Conversation\")\n",
        "\n",
        "    # Use a State component to store chat history\n",
        "    chat_history_state = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        chat_input = gr.Textbox(label=\"Your question\", scale=4)\n",
        "        submit_btn = gr.Button(\"Send\", scale=1)\n",
        "\n",
        "    # Modify the click function to pass chat_history_state and update chatbot_output\n",
        "    submit_btn.click(\n",
        "        respond,\n",
        "        inputs=[chat_input, chat_history_state, domain],\n",
        "        outputs=[chatbot_output, chat_input]\n",
        "    )\n",
        "\n",
        "    # Add event listener to chat_input to trigger submit_btn.click on Enter key press\n",
        "    chat_input.submit(\n",
        "        respond,\n",
        "        inputs=[chat_input, chat_history_state, domain],\n",
        "        outputs=[chatbot_output, chat_input]\n",
        "    )\n",
        "\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "n_5LH3fpqsya",
        "outputId": "cb19b14a-3e5b-4d62-b84e-fc9fc37a097a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3923110952.py:48: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot_output = gr.Chatbot(label=\"Conversation\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1dd22282dc16f53c72.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1dd22282dc16f53c72.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented code below because Deepseek 33B loads 37B params out of 685B total,\n",
        "# that demands serious memory capacity\n",
        "#from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Load DeepSeek V3.1 from Hugging Face\n",
        "#model_id = \"deepseek-ai/deepseek-coder-33b-instruct-GPTQ\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\")\n",
        "\n",
        "# Create chatbot pipeline\n",
        "#chatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\"\"\" Below code never worked , could not install GPTQ model on CUDA\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "import torch\n",
        "\n",
        "model_id = \"deepseek-ai/deepseek-coder-33b-instruct-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3N5f4RsMkbUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save this as app.py and run with: streamlit run app.py\n",
        "\"\"\"\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"DeepSeek Chatbot\")\n",
        "domain = st.selectbox(\"Choose domain\", [\"healthcare\", \"finance\"])\n",
        "user_input = st.text_input(\"Ask your question\")\n",
        "\n",
        "if user_input:\n",
        "    prompt = build_prompt(user_input, domain)\n",
        "    response = get_response(prompt)\n",
        "    st.write(response)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Spw4IKZQkrfp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}