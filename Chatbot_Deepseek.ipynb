{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOs2apjFMJS0qf9qlqots7Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29e94d001c844ba38e4d0959665dd1f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5341a357e6b94be8a5d7e8e95fd362b3",
              "IPY_MODEL_1f3f527a62bd48ccb14cd284afc5d052",
              "IPY_MODEL_0f5d2bd671a94636a263c4fd9f4378fc"
            ],
            "layout": "IPY_MODEL_9e1e091699484769aabfb417da15b956"
          }
        },
        "5341a357e6b94be8a5d7e8e95fd362b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dc0fdbd0b83468ab5547fda8fbbef39",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_45da466133324f31b97fe659b9b4ea33",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "1f3f527a62bd48ccb14cd284afc5d052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f2448ce010f4a4c9b171fda5cecc92a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_217da95d1b784bb5943bec118f7a10de",
            "value": 2
          }
        },
        "0f5d2bd671a94636a263c4fd9f4378fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83a04c463ac44c3f989d0aca88fee2e8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bb09cc763be04f6caa6bc0ed73f3997a",
            "value": "â€‡2/2â€‡[00:04&lt;00:00,â€‡â€‡1.85s/it]"
          }
        },
        "9e1e091699484769aabfb417da15b956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dc0fdbd0b83468ab5547fda8fbbef39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45da466133324f31b97fe659b9b4ea33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f2448ce010f4a4c9b171fda5cecc92a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "217da95d1b784bb5943bec118f7a10de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83a04c463ac44c3f989d0aca88fee2e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb09cc763be04f6caa6bc0ed73f3997a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadicodes/LLM-Chat/blob/main/Chatbot_Deepseek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQdiB9Hpjncv",
        "outputId": "cfe6326b-61f4-49e3-b635-09d3a6dd2315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files removed: 14 (3.2 MB)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "# !pip install transformers accelerate streamlit --quiet\n",
        "!pip cache purge\n",
        "\n",
        "!pip install --upgrade pip setuptools wheel\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y build-essential libssl-dev libffi-dev python3-dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDDhbwTevWWT",
        "outputId": "7b296b1f-370d-441c-84aa-90ecd3ecc43f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (185.125.190.82)] [Connecting to cloud.r-\r                                                                               \rGet:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [3 InRelease 8,380 B/128 kB 7%] [Connecting to security.ubuntu.com (185.125.\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,937 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,172 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,569 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,575 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,430 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,775 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,253 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,271 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,235 kB]\n",
            "Fetched 34.6 MB in 3s (13.6 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libffi-dev is already the newest version (3.4.2-4).\n",
            "libffi-dev set to manually installed.\n",
            "libssl-dev is already the newest version (3.0.2-0ubuntu1.19).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04.1).\n",
            "python3-dev set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate text-generation"
      ],
      "metadata": {
        "id": "5JG6o_FzvOq5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc647c28",
        "outputId": "c2d21404-8c87-4f7e-e834-88f5b8ea3f67"
      },
      "source": [
        "# check disk space\n",
        "!df -h /\n",
        "!du -sh /root/* /usr/* /content/* 2>/dev/null"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         113G   40G   74G  35% /\n",
            "199M\t/usr/bin\n",
            "25M\t/usr/colab\n",
            "20K\t/usr/etc\n",
            "4.0K\t/usr/games\n",
            "2.0M\t/usr/grte\n",
            "271M\t/usr/include\n",
            "5.5G\t/usr/lib\n",
            "15M\t/usr/lib32\n",
            "4.0K\t/usr/lib64\n",
            "437M\t/usr/lib64-nvidia\n",
            "1.7M\t/usr/libexec\n",
            "4.0K\t/usr/libx32\n",
            "26G\t/usr/local\n",
            "8.7M\t/usr/sbin\n",
            "502M\t/usr/share\n",
            "141M\t/usr/src\n",
            "55M\t/content/sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "29e94d001c844ba38e4d0959665dd1f9",
            "5341a357e6b94be8a5d7e8e95fd362b3",
            "1f3f527a62bd48ccb14cd284afc5d052",
            "0f5d2bd671a94636a263c4fd9f4378fc",
            "9e1e091699484769aabfb417da15b956",
            "3dc0fdbd0b83468ab5547fda8fbbef39",
            "45da466133324f31b97fe659b9b4ea33",
            "4f2448ce010f4a4c9b171fda5cecc92a",
            "217da95d1b784bb5943bec118f7a10de",
            "83a04c463ac44c3f989d0aca88fee2e8",
            "bb09cc763be04f6caa6bc0ed73f3997a"
          ]
        },
        "id": "3c370015",
        "outputId": "ce1b91c8-c6e7-467e-d77f-b3679c4b1303"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "# Create chatbot pipeline\n",
        "chatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29e94d001c844ba38e4d0959665dd1f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check disk space\n",
        "!df -h /\n",
        "!du -sh /root/* /usr/* /content/* 2>/dev/null"
      ],
      "metadata": {
        "id": "TpzgNTtz06yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the GPU allocated in virtual environment\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "jr9veaAon2KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(user_input, domain=\"healthcare\"):\n",
        "    system_prompt = f\"You are a helpful assistant for {domain} customer service. Be accurate, empathetic, funny and concise.\"\n",
        "    return f\"{system_prompt}\\nUser: {user_input}\\nAssistant:\"\n"
      ],
      "metadata": {
        "id": "k5aSx3k_kgFP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(prompt, max_tokens=512):\n",
        "    response = chatbot(prompt, max_new_tokens=max_tokens, do_sample=True, temperature=0.7)\n",
        "    return response[0]['generated_text']\n"
      ],
      "metadata": {
        "id": "Id9RxUVukdoc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"I need help understanding my insurance coverage for a recent surgery.\"\n",
        "prompt = build_prompt(user_input, domain=\"healthcare\")\n",
        "response = get_response(prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "aIXd0jXIklHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df80ec3-bbf6-40e9-f56c-1f9d4e09a472"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful assistant for healthcare customer service. Be accurate, empathetic, and concise.\n",
            "User: I need help understanding my insurance coverage for a recent surgery.\n",
            "Assistant: I'm sorry for any confusion, but as an AI focused on healthcare, I'm unable to assist with insurance coverage questions. I'd recommend reaching out to your insurance company or a healthcare professional for assistance with these matters.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def build_prompt(user_input, domain=\"healthcare\"):\n",
        "    system_prompt = f\"You are a helpful assistant for {domain} customer service. Be accurate, empathetic, funny and concise.\"\n",
        "    return f\"{system_prompt}\\nUser: {user_input}\\nAssistant:\"\n",
        "\n",
        "def get_response(prompt, max_tokens=512):\n",
        "    # Assuming 'chatbot' pipeline is already defined and loaded\n",
        "    # from the previous cell (3c370015)\n",
        "    response = chatbot(prompt, max_new_tokens=max_tokens, do_sample=True, temperature=0.7)\n",
        "    # Extract the generated text from the response.\n",
        "    # The pipeline output includes the input prompt, so we need to remove it.\n",
        "    generated_text = response[0]['generated_text']\n",
        "    # Find the start of the assistant's response (after the prompt)\n",
        "    assistant_start = generated_text.find(\"Assistant:\")\n",
        "    if assistant_start != -1:\n",
        "        return generated_text[assistant_start + len(\"Assistant:\"):].strip()\n",
        "    return generated_text.strip()\n",
        "\n",
        "\n",
        "def respond(user_input, chat_history, selected_domain):\n",
        "    if not user_input:\n",
        "        return chat_history, \"\"\n",
        "\n",
        "    prompt = build_prompt(user_input, selected_domain)\n",
        "    bot_response = get_response(prompt)\n",
        "\n",
        "    # Append user input and bot response to chat history\n",
        "    chat_history.append((user_input, bot_response))\n",
        "\n",
        "    # Return the updated chat history and clear the input box\n",
        "    return chat_history, \"\"\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ðŸ’¬ DeepSeek Chatbot for Finance & Healthcare - Rajesh Aadi\")\n",
        "\n",
        "    domain = gr.Radio([\"finance\", \"healthcare\"], label=\"Choose Domain\", value=\"finance\")\n",
        "\n",
        "    # Use Chatbot component to display conversation history\n",
        "    chatbot_output = gr.Chatbot(label=\"Conversation\")\n",
        "\n",
        "    # Use a State component to store chat history\n",
        "    chat_history_state = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        chat_input = gr.Textbox(label=\"Your question\", scale=4)\n",
        "        submit_btn = gr.Button(\"Send\", scale=1)\n",
        "\n",
        "    # Modify the click function to pass chat_history_state and update chatbot_output\n",
        "    submit_btn.click(\n",
        "        respond,\n",
        "        inputs=[chat_input, chat_history_state, domain],\n",
        "        outputs=[chatbot_output, chat_input]\n",
        "    )\n",
        "\n",
        "    # Add event listener to chat_input to trigger submit_btn.click on Enter key press\n",
        "    chat_input.submit(\n",
        "        respond,\n",
        "        inputs=[chat_input, chat_history_state, domain],\n",
        "        outputs=[chatbot_output, chat_input]\n",
        "    )\n",
        "\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "n_5LH3fpqsya",
        "outputId": "afad9b45-c150-4860-e1ba-39143f0bb445"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3390332244.py:41: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot_output = gr.Chatbot(label=\"Conversation\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9795c5212f03f778a7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9795c5212f03f778a7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented code below because Deepseek 33B loads 37B params out of 685B total,\n",
        "# that demands serious memory capacity\n",
        "#from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Load DeepSeek V3.1 from Hugging Face\n",
        "#model_id = \"deepseek-ai/deepseek-coder-33b-instruct-GPTQ\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\")\n",
        "\n",
        "# Create chatbot pipeline\n",
        "#chatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\"\"\" Below code never worked , could not install GPTQ model on CUDA\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "import torch\n",
        "\n",
        "model_id = \"deepseek-ai/deepseek-coder-33b-instruct-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3N5f4RsMkbUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save this as app.py and run with: streamlit run app.py\n",
        "\"\"\"\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"DeepSeek Chatbot\")\n",
        "domain = st.selectbox(\"Choose domain\", [\"healthcare\", \"finance\"])\n",
        "user_input = st.text_input(\"Ask your question\")\n",
        "\n",
        "if user_input:\n",
        "    prompt = build_prompt(user_input, domain)\n",
        "    response = get_response(prompt)\n",
        "    st.write(response)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Spw4IKZQkrfp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}